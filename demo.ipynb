{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.20.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2000\n",
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(D, N)\n",
    "w = np.random.rand(N)\n",
    "z = np.random.rand(N)>0.1\n",
    "w = w*z\n",
    "w = w/w.sum()\n",
    "y = X.dot(w).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X.astype(np.float32))\n",
    "y = torch.tensor(y.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta dataset: used for training the lambda producer\n",
    "X_meta = X[:1900].cuda()\n",
    "y_meta = y[:1900].cuda()\n",
    "# training set: used for training the regression model\n",
    "X_trn = X[1150:1900].cuda()\n",
    "y_trn = y[1150:1900].cuda()\n",
    "# test test\n",
    "X_tst = X[1900:].cuda()\n",
    "y_tst = y[1900:].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda_Producer(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(Lambda_Producer, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(D, 1)\n",
    "        self.fc1 = nn.Linear(D*3, D)\n",
    "        self.fc2 = nn.Linear(D, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        N = x.shape[1]\n",
    "        x = x.t()[:,None,:]\n",
    "        v = self.self_attention(x, x, x, need_weights=False)\n",
    "        v = torch.cat([x.squeeze(1), v[0].squeeze(1), y.repeat(1, N).t()], axis=1)\n",
    "        v = self.fc1(v)\n",
    "        v = torch.relu(v)\n",
    "        v = self.fc2(v)\n",
    "        v = torch.exp(v)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qp_solver_cvxpy(lambdas, X, y):\n",
    "    \n",
    "    N = X.shape[1]\n",
    "    y = y.flatten()\n",
    "\n",
    "    P = 2*X.T.dot(X)\n",
    "    q = -2*X.T.dot(y) + lambdas\n",
    "    G = -np.eye(N)\n",
    "    h = np.zeros(N)\n",
    "    A = np.ones((1,N))\n",
    "    b = np.array([1.0])\n",
    "\n",
    "    w = cp.Variable(N)\n",
    "    \n",
    "    obj = cp.Minimize((1/2)*cp.quad_form(w, P) + q @ w)\n",
    "    \n",
    "    constraints = [A @ w == b, G @ w <= h]\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "    prob.solve()\n",
    "    \n",
    "    return w.value, constraints[0].dual_value, constraints[1].dual_value, P, q\n",
    "\n",
    "\n",
    "def diff_w_lambdas(w, a2, P, grad_output):\n",
    "    D = w.shape[0]\n",
    "    dfds = torch.vstack([torch.hstack([P, \n",
    "                                       torch.ones((D,1)).type_as(P), \n",
    "                                       -torch.eye(D).type_as(P)]),\n",
    "                         torch.hstack([torch.ones((1,D)).type_as(P), \n",
    "                                       torch.zeros((1,1)).type_as(P), \n",
    "                                       torch.zeros((1,D)).type_as(P)]),\n",
    "                         torch.hstack([-torch.diag(a2), \n",
    "                                       torch.zeros((D,1)).type_as(P), \n",
    "                                       -torch.diag(w)])])\n",
    "    partial = -torch.linalg.solve(dfds.t(), \n",
    "                                  torch.vstack([grad_output.view(-1,1), \n",
    "                                                torch.zeros((D+1,1)).type_as(P)]))\n",
    "    return partial[:D].view(-1)\n",
    "\n",
    "\n",
    "class QPSolver(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, lambdas, X, y):\n",
    "        X_np = X.detach().cpu().numpy()\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        lambdas_np = lambdas.detach().cpu().numpy()\n",
    "        w, _, a2, P, _ = qp_solver_cvxpy(lambdas_np, X_np, y_np)\n",
    "        w = torch.from_numpy(w).type_as(X)\n",
    "        a2 = torch.from_numpy(a2).type_as(X)\n",
    "        P = torch.from_numpy(P).type_as(X)\n",
    "        ctx.save_for_backward(w, a2, P)\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        w, a2, P = ctx.saved_tensors\n",
    "        partial = diff_w_lambdas(w, a2, P, grad_output)\n",
    "        return partial, None, None\n",
    "    \n",
    "qp_solver = QPSolver.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 750: meta_trn/trn length\n",
    "lambda_producer = Lambda_Producer(750).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(lambda_producer.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6017117e-05\n",
      "1.1148499e-05\n",
      "1.3522909e-05\n",
      "1.2993502e-05\n",
      "1.1634161e-05\n",
      "4.7232315e-06\n",
      "8.846097e-06\n",
      "7.21329e-06\n",
      "7.385661e-06\n",
      "2.0199393e-06\n",
      "5.9200775e-06\n",
      "6.414053e-06\n",
      "4.8644447e-06\n",
      "7.188181e-06\n",
      "5.5105384e-06\n",
      "4.0770637e-06\n",
      "3.7563307e-06\n",
      "3.694881e-06\n",
      "3.5604696e-06\n",
      "2.4388557e-06\n"
     ]
    }
   ],
   "source": [
    "# now we train the lambda producer (i.e., meta model)\n",
    "# note that, the training set contains 750 days data (that's the period that we use for training the regression model)\n",
    "# and the testing set contains 100 days data (that's the period that we buy at beginning and hold) \n",
    "# thus, we should always sample 750 + 100 days data from meta dataset, and build the meta_train set and meta_test set.\n",
    "\n",
    "for I in range(200):\n",
    "    rnd_starting_idx = np.random.choice(1050)\n",
    "    X_meta_trn = X_meta[rnd_starting_idx:(rnd_starting_idx+750)]\n",
    "    y_meta_trn = y_meta[rnd_starting_idx:(rnd_starting_idx+750)]\n",
    "    X_meta_tst = X_meta[(rnd_starting_idx+750):(rnd_starting_idx+850)]\n",
    "    y_meta_tst = y_meta[(rnd_starting_idx+750):(rnd_starting_idx+850)]\n",
    "    lambdas = lambda_producer(X_meta_trn, y_meta_trn).flatten()\n",
    "    w_hat = qp_solver(lambdas, X_meta_trn, y_meta_trn)\n",
    "    meta_loss = ((X_meta_tst.mm(w_hat.view(-1,1))-y_meta_tst)**2).mean()\n",
    "    opt.zero_grad()\n",
    "    meta_loss.backward()\n",
    "    opt.step()\n",
    "    if I % 10 == 0:\n",
    "        print(meta_loss.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = lambda_producer(X_trn, y_trn).flatten().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat = qp_solver(lambdas, X_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = ((X_tst.mm(w_hat.view(-1,1))-y_tst)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3518e-06, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
